{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a44dfd39",
   "metadata": {},
   "source": [
    "# FastText Embedding Aprroach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b48c88a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from khmernltk import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "# from google.colab import drive\n",
    "from gensim.models import FastText\n",
    "from gensim.test.utils import common_texts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699208e7",
   "metadata": {},
   "source": [
    "### Function to clean and tokenize text (Preprocessing)\n",
    "- Remove spcial symbols\n",
    "- Tokenize by split the words with space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0916fc52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clean and tokenize full article text\n",
    "def clean_and_tokenize(text):\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Filter tokens: keep words, numbers, and common punctuation (.,!?)\n",
    "    cleaned_tokens = []\n",
    "    for token in tokens:\n",
    "        # Allow Khmer text (letters), numbers, spaces, and punctuation (.,!?)\n",
    "        if re.match(r'^[\\u1780-\\u17FF\"]+$', token):  # Khmer Unicode range + numbers and some punctuation\n",
    "            cleaned_tokens.append(token)\n",
    "\n",
    "    # Join the cleaned tokens and remove multiple spaces\n",
    "    cleaned_text = ' '.join(cleaned_tokens)\n",
    "\n",
    "    # Replace multiple spaces with a single space\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2476735c",
   "metadata": {},
   "source": [
    "Load Data from folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35b84d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "file_names = []\n",
    "training_data = '/Users/mac/Desktop/thesis/clean_data1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5b4a31",
   "metadata": {},
   "source": [
    "- Load the data into documents list for later embedding step\n",
    "- Extract the Summary and Full Article and put in dataframe for easy access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24edd777",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "documents = []\n",
    "\n",
    "for file in sorted(os.listdir(training_data)):\n",
    "    if file.endswith(\".txt\"):\n",
    "        file_path = os.path.join(training_data, file)\n",
    "\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        summary = None\n",
    "        full_article = None\n",
    "\n",
    "        for i, line in enumerate(lines):\n",
    "            if line.strip() == \"Summary:\":\n",
    "                summary = lines[i + 1].strip()\n",
    "            elif line.strip() == \"Full Article:\":\n",
    "                full_article = ''.join(lines[i + 1:]).strip()\n",
    "                break\n",
    "\n",
    "        if summary and full_article:\n",
    "            article_name = os.path.splitext(file)[0]  # Remove \".txt\"\n",
    "            documents.append({\n",
    "                'name': article_name,\n",
    "                'summary': clean_and_tokenize(summary),\n",
    "                'full_text': clean_and_tokenize(full_article),\n",
    "            })\n",
    "            file_names.append(file)\n",
    "        else:\n",
    "            print(f\"Missing summary or full article in {file}. Skipping.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2589caf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>summary</th>\n",
       "      <th>full_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>article_1</td>\n",
       "      <td>ស្ត្រី ជា បុគ្គល ដ៏ សំខាន់ ក្នុង រួមចំណែក ដំណើ...</td>\n",
       "      <td>កាលពី អតីតកាល កិច្ចការងារ និង មុខ តំណែង ក្នុង ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>article_100</td>\n",
       "      <td>អ្នកនាំពាក្យ ក្រសួង ការបរទេស អាម៉េរិក លោក កាលព...</td>\n",
       "      <td>ដំណើរទស្សនកិច្ច ទៅកាន់ ទីក្រុង សេអ៊ូល ជា ផ្នែក...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>article_1000</td>\n",
       "      <td>ព្រឹទ្ធសភា អាមេរិក នៅ ថ្ងៃទី ៩ ខែកុម្ភៈ បាន យល...</td>\n",
       "      <td>សវនាការ ដក តំណែង លើក ទី ពីរ ប្រឆាំង អតីត ប្រធា...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>article_10000</td>\n",
       "      <td>សកម្មភាព លក់ លំនៅឋាន ដែល មាន ផ្ទះល្វែង វីឡា ភ្...</td>\n",
       "      <td>របាយការណ៍ ដែល យោង តាម និយ័តករ អាជីវករ អចលន វត្...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>article_10001</td>\n",
       "      <td>ភ្នំពេញ៖ លោក វ៉ាង យី រដ្ឋមន្រ្តី ការបរទេស ចិន ...</td>\n",
       "      <td>ក្នុង ជំនួប នេះ លោក សុខ ចិន្តា សោភា បាន រំលេច ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10020</th>\n",
       "      <td>article_9995</td>\n",
       "      <td>របាយការណ៍ ថ្មី មួយ ដែល សិក្សា ដោយ អង្គការ លីកា...</td>\n",
       "      <td>យោង តាម របាយការណ៍ នេះ អ្នក ខ្ចី ប្រាក់ ជាង ៣ យ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10021</th>\n",
       "      <td>article_9996</td>\n",
       "      <td>៧ខែ មក នេះ កម្ពុជា ទទួលបាន ភ្ញៀវ ទេសចរ បរទេស ស...</td>\n",
       "      <td>ក្រសួង ទេសចរណ៍ បាន បញ្ជាក់ ថា បន្ទាប់ពី ថៃ ទេស...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10022</th>\n",
       "      <td>article_9997</td>\n",
       "      <td>រយៈពេល ៧ខែ មក នេះ ការនាំចេញ របស់ កម្ពុជា ទៅកាន...</td>\n",
       "      <td>គឺជា កិច្ចព្រម ពៀង ភាពជាដៃគូ សេដ្ឋកិច្ច គ្រប់ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10023</th>\n",
       "      <td>article_9998</td>\n",
       "      <td>ធនាគារ ជាតិ នៃ កម្ពុជា ប្រកាស ដាក់ ដេញថ្លៃ ប្រ...</td>\n",
       "      <td>បើ តាម សេចក្តីជូនដំណឹង របស់ ធនាគារ ជាតិ ការដេញ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10024</th>\n",
       "      <td>article_9999</td>\n",
       "      <td>៧ខែ មក នេះ ការធ្វើ ពាណិជ្ជកម្ម រវាង កម្ពុជា ជា...</td>\n",
       "      <td>យោង តាម ទិន្នន័យ ពី អគ្គ នាយកដ្ឋាន គយ និង រដ្ឋ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10025 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                name                                            summary  \\\n",
       "0          article_1  ស្ត្រី ជា បុគ្គល ដ៏ សំខាន់ ក្នុង រួមចំណែក ដំណើ...   \n",
       "1        article_100  អ្នកនាំពាក្យ ក្រសួង ការបរទេស អាម៉េរិក លោក កាលព...   \n",
       "2       article_1000  ព្រឹទ្ធសភា អាមេរិក នៅ ថ្ងៃទី ៩ ខែកុម្ភៈ បាន យល...   \n",
       "3      article_10000  សកម្មភាព លក់ លំនៅឋាន ដែល មាន ផ្ទះល្វែង វីឡា ភ្...   \n",
       "4      article_10001  ភ្នំពេញ៖ លោក វ៉ាង យី រដ្ឋមន្រ្តី ការបរទេស ចិន ...   \n",
       "...              ...                                                ...   \n",
       "10020   article_9995  របាយការណ៍ ថ្មី មួយ ដែល សិក្សា ដោយ អង្គការ លីកា...   \n",
       "10021   article_9996  ៧ខែ មក នេះ កម្ពុជា ទទួលបាន ភ្ញៀវ ទេសចរ បរទេស ស...   \n",
       "10022   article_9997  រយៈពេល ៧ខែ មក នេះ ការនាំចេញ របស់ កម្ពុជា ទៅកាន...   \n",
       "10023   article_9998  ធនាគារ ជាតិ នៃ កម្ពុជា ប្រកាស ដាក់ ដេញថ្លៃ ប្រ...   \n",
       "10024   article_9999  ៧ខែ មក នេះ ការធ្វើ ពាណិជ្ជកម្ម រវាង កម្ពុជា ជា...   \n",
       "\n",
       "                                               full_text  \n",
       "0      កាលពី អតីតកាល កិច្ចការងារ និង មុខ តំណែង ក្នុង ...  \n",
       "1      ដំណើរទស្សនកិច្ច ទៅកាន់ ទីក្រុង សេអ៊ូល ជា ផ្នែក...  \n",
       "2      សវនាការ ដក តំណែង លើក ទី ពីរ ប្រឆាំង អតីត ប្រធា...  \n",
       "3      របាយការណ៍ ដែល យោង តាម និយ័តករ អាជីវករ អចលន វត្...  \n",
       "4      ក្នុង ជំនួប នេះ លោក សុខ ចិន្តា សោភា បាន រំលេច ...  \n",
       "...                                                  ...  \n",
       "10020  យោង តាម របាយការណ៍ នេះ អ្នក ខ្ចី ប្រាក់ ជាង ៣ យ...  \n",
       "10021  ក្រសួង ទេសចរណ៍ បាន បញ្ជាក់ ថា បន្ទាប់ពី ថៃ ទេស...  \n",
       "10022  គឺជា កិច្ចព្រម ពៀង ភាពជាដៃគូ សេដ្ឋកិច្ច គ្រប់ ...  \n",
       "10023  បើ តាម សេចក្តីជូនដំណឹង របស់ ធនាគារ ជាតិ ការដេញ...  \n",
       "10024  យោង តាម ទិន្នន័យ ពី អគ្គ នាយកដ្ឋាន គយ និង រដ្ឋ...  \n",
       "\n",
       "[10025 rows x 3 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(documents)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9771ae0a",
   "metadata": {},
   "source": [
    "Split data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edc993de",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4fbddab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the training and testing data to text files for FastText\n",
    "train['full_text'].to_csv('train.txt', index=False, header=False, sep='\\n')\n",
    "test['summary'].to_csv('test.txt', index=False, header=False, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9221c08",
   "metadata": {},
   "source": [
    "Train Fasttext to learn the words Then Embedd the entire documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b617ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 2M words\n",
      "Number of words:  9680\n",
      "Number of labels: 0\n",
      "Progress: 100.0% words/sec/thread:  118675 lr:  0.000000 avg.loss:  2.175753 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Train the FastText model on the full text\n",
    "model = fasttext.train_unsupervised('train.txt')\n",
    "\n",
    "# Save the trained model for later use\n",
    "model.save_model('khmer_model.bin')\n",
    "\n",
    "# Precompute article vectors only once\n",
    "article_vectors = [model.get_sentence_vector(text) for text in df['full_text']]\n",
    "article_vectors = np.array(article_vectors)  # Convert to numpy array for faster similarity calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3188539b",
   "metadata": {},
   "source": [
    "Evalute the model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "54eaeb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieval@10 Accuracy: 59.90\n",
      "804\n"
     ]
    }
   ],
   "source": [
    "correct_in_top10 = 0\n",
    "not_cor = 0\n",
    "\n",
    "# Iterate over each row in the test set\n",
    "for _, row in test.iterrows():\n",
    "    query_summary = row['summary']\n",
    "    query_vector = model.get_sentence_vector(query_summary)\n",
    "    similarities = cosine_similarity([query_vector], article_vectors)\n",
    "\n",
    "    # Get indices of the top 10 most similar articles\n",
    "    top_10_indices = similarities[0].argsort()[-10:][::-1]\n",
    "    top_10_articles = df.iloc[top_10_indices]\n",
    "\n",
    "    # Check if the label (name) of the current test article is in the top 10 articles\n",
    "    top_10_labels = top_10_articles['name'].values\n",
    "    if row['name'] in top_10_labels:\n",
    "        correct_in_top10 += 1\n",
    "    else:\n",
    "        not_cor += 1\n",
    "\n",
    "# Calculate the accuracy\n",
    "accuracy = correct_in_top10 / len(test)\n",
    "print(f\"Retrieval@10 Accuracy: {accuracy * 100:.2f}\")\n",
    "print(not_cor)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
